{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea49a01",
   "metadata": {},
   "source": [
    "# Getting Started with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89595db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b32e7",
   "metadata": {},
   "source": [
    "## 1. Initializing tensors from lists and arrays\n",
    "Initialize a tensor from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3438b12",
   "metadata": {},
   "source": [
    "Initialize a tensor from a `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c952f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5266ec",
   "metadata": {},
   "source": [
    "These should be identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99dd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data == x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b24231",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "arr = np.random.uniform(size=(3,3))\n",
    "x_rand = torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d77e54",
   "metadata": {},
   "source": [
    "Tensor slicing and indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row\n",
    "x_rand[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370276d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first column\n",
    "x_rand[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last row\n",
    "x_rand[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last column\n",
    "x_rand[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4971e",
   "metadata": {},
   "source": [
    "## 2. Native tensor initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a785d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates an empty tensor of floats\n",
    "# note: that you can't specify another dtype\n",
    "torch.Tensor(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by contrast with torch.empty \n",
    "# one can create tensors of arbitrary dtype\n",
    "torch.empty(2,2, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e176d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(0,10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b229aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(22)\n",
    "torch.rand((3,3))\n",
    "\n",
    "# what is 'torch.random.seed()'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227c5a1",
   "metadata": {},
   "source": [
    "Reshape the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(1,5)\n",
    "a.reshape((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.reshape(a, (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(22)\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844894f",
   "metadata": {},
   "source": [
    "Attributes of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f478ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336449d9",
   "metadata": {},
   "source": [
    "Torch data types can be specified at initialization or tensors can be converted using `to()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4481b",
   "metadata": {},
   "source": [
    "## 3. Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cb179",
   "metadata": {},
   "source": [
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\n",
    "``.to`` method (after checking for GPU availability). Keep in mind that copying large tensors\n",
    "across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0404dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefbe69",
   "metadata": {},
   "source": [
    "Arithmetic operations on tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ca1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # additon allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors\n",
    "\n",
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "# uncomment this line to get a runtime error\n",
    "# r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02a707",
   "metadata": {},
   "source": [
    "Each element of the tensor `twos` raised to the power specified in the second tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ddec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twos = torch.ones(2,2)*2\n",
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ba843",
   "metadata": {},
   "source": [
    "Tensor broadcasting performs an operation between tensors of similar shapes by repeating the elements of the tensor with lower dimensions. Broadcasting is important for deep learning, for example multiplying a tensor of learned weights by a batch of input tensors, as this operation is applied to each instance in the batch separately, and returning a tensor of identical shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebadfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae432826",
   "metadata": {},
   "source": [
    "The rules for broadcasting are:\n",
    "\n",
    "+ Each tensor must have at least one dimension - no empty tensors.\n",
    "+ **Comparing the dimension sizes of the two tensors, going from last to first:**\n",
    "    + Each dimension must be equal, or \n",
    "    + One of the dimensions must be of size 1, or\n",
    "    + The dimension does not exist in one of the tensors\n",
    "\n",
    "More examples of broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec26b32",
   "metadata": {},
   "source": [
    "Here’s a small sample of the mathematical operations available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195daf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and linear algebra operations like determinant and singular value decomposition\n",
    "print('\\nDeterminant of r:')\n",
    "print(torch.det(r))\n",
    "print('\\nSingular value decomposition of r:')\n",
    "print(torch.svd(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fce09",
   "metadata": {},
   "source": [
    "Get unique values of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464b414",
   "metadata": {},
   "source": [
    "Concatenate tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210dcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.cat([tensor, tensor], dim=0)\n",
    "print(t2)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b925b5",
   "metadata": {},
   "source": [
    "`torch.stack` creates an extra dimension for accumulating tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75241a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.stack([tensor, tensor], dim=0)\n",
    "print(t3)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002830a",
   "metadata": {},
   "source": [
    "Matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 == y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46277",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 == y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82164b1a",
   "metadata": {},
   "source": [
    "Element-wise product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387d84d",
   "metadata": {},
   "source": [
    "To add a dummy dimension to a tensor, use `unsqueeze`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f59cd8",
   "metadata": {},
   "source": [
    "To reshape a tensor use `reshape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86885b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a005b6",
   "metadata": {},
   "source": [
    "If you have a one-element tensor, for example by aggregating all\n",
    "values of a tensor into one value, you can convert it to a Python\n",
    "numerical value using ``item()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c8c53",
   "metadata": {},
   "source": [
    "**In-place operations**\n",
    "Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix.\n",
    "For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fb614",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\n",
    "     of history. Hence, their use is discouraged.</p></div>\n",
    "     \n",
    "As with any object in Python, assigning a tensor to a variable makes the variable a label of the tensor, and does not copy it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f164159",
   "metadata": {},
   "source": [
    "To create a separate copy, the `clone()` method can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61516e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910f37f",
   "metadata": {},
   "source": [
    "**If your source tensor has autograd, enabled then so will the clone.** In many cases, this will be what you want. For example, if your model has multiple computation paths in its forward() method, and both the original tensor and its clone contribute to the model’s output, then to enable model learning you want autograd turned on for both tensors. \n",
    "\n",
    "On the other hand, if you’re doing a computation where neither the original tensor nor its clone need to track gradients, then as long as the source tensor has autograd turned off, you’re good to go.\n",
    "\n",
    "There is a third case, though: Imagine you’re performing a computation in your model’s forward() function, where gradients are turned on for everything by default, but you want to pull out some values mid-stream to generate some metrics. In this case, you don’t want the cloned copy of your source tensor to track gradients - performance is improved with autograd’s history tracking turned off. For this, you can use the .detach() method on the source tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7df15",
   "metadata": {},
   "source": [
    "Tensor to NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01698bc4",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26961a63",
   "metadata": {},
   "source": [
    "numpy array to tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd22f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e6c9b",
   "metadata": {},
   "source": [
    "Changes in the NumPy array reflects in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a013b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8f933",
   "metadata": {},
   "source": [
    "## 4. Automatic differentiation with `torch.autograd`\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine\n",
    "called ``torch.autograd``. It supports automatic computation of gradient for any\n",
    "computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input ``x``,\n",
    "parameters ``w`` and ``b``, and some loss function. It can be defined in\n",
    "PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # with logits! (no softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da12b6",
   "metadata": {},
   "source": [
    "A function that we apply to tensors to construct computational graph is\n",
    "in fact an object of class ``Function``. This object knows how to\n",
    "compute the function in the *forward* direction, and also how to compute\n",
    "its derivative during the *backward propagation* step. A reference to\n",
    "the backward propagation function is stored in ``grad_fn`` property of a\n",
    "tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9b4e7",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to\n",
    "compute the derivatives of our loss function with respect to parameters,\n",
    "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
    "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of\n",
    "``x`` and ``y``. To compute those derivatives, we call\n",
    "``loss.backward()``, and then retrieve the values from ``w.grad`` and\n",
    "``b.grad``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1af9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>\n",
    "\n",
    "+ We can only obtain the ``grad`` properties for the leaf\n",
    "    nodes of the computational graph, which have ``requires_grad`` property\n",
    "    set to ``True``. For all other nodes in our graph, gradients will not be\n",
    "    available.\n",
    "    \n",
    "+ We can only perform gradient calculations using\n",
    "    ``backward`` once on a given graph, for performance reasons. If we need\n",
    "    to do several ``backward`` calls on the same graph, we need to pass\n",
    "    ``retain_graph=True`` to the ``backward`` call.</p></div>\n",
    "    \n",
    "    \n",
    "By default, all tensors with ``requires_grad=True`` are tracking their\n",
    "computational history and support gradient computation. After we have\n",
    "trained the model and just want to apply it to some test data, i.e. we\n",
    "only want to do *forward* computations through the network. We can stop\n",
    "tracking computations by surrounding our computation code with\n",
    "``torch.no_grad()`` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923556e5",
   "metadata": {},
   "source": [
    "Another way to achieve the same result is to use the ``detach()`` method\n",
    "on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ff253",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374c324",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "  - To mark some parameters in your neural network as **frozen parameters**. This is\n",
    "    a very common scenario for finetuning a pretrained network\n",
    "  - To **speed up computations** when you are only doing forward pass, because computations on tensors that do\n",
    "    not track gradients would be more efficient.\n",
    "    \n",
    "\n",
    "In many cases, we have a scalar loss function, and we need to compute\n",
    "the gradient with respect to some parameters. However, there are cases\n",
    "when the output function is an arbitrary tensor. In this case, PyTorch\n",
    "allows you to compute so-called **Jacobian product**, and not the actual\n",
    "gradient.\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of\n",
    "$\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian\n",
    "matrix**:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, PyTorch allows you to\n",
    "compute **Jacobian Product** $v^T\\cdot J$ for a given input vector\n",
    "$v=(v_1 \\dots v_m)$. This is achieved by calling ``backward`` with\n",
    "$v$ as an argument. The size of $v$ should be the same as\n",
    "the size of the original tensor, with respect to which we want to\n",
    "compute the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d901ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input: {inp}\")\n",
    "print(f\"Output: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9c8fc",
   "metadata": {},
   "source": [
    "When we call ``backward`` for the second time with the same\n",
    "argument, the value of the gradient is different. This happens because\n",
    "when doing ``backward`` propagation, PyTorch **accumulates the\n",
    "gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05032cc7",
   "metadata": {},
   "source": [
    "Another example of autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2919fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdd040",
   "metadata": {},
   "source": [
    "When we print `b`, we see an indicator that it is tracking its computation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e24895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fed7af",
   "metadata": {},
   "source": [
    "This `grad_fn` gives us a hint that when we execute the backpropagation step and compute gradients, we’ll need to compute the derivative of $\\sin(x)$ for all this tensor’s inputs.\n",
    "\n",
    "Let’s perform some more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1313a",
   "metadata": {},
   "source": [
    "Finally, let’s compute a single-element output. When you call .backward() on a tensor with no arguments, it expects the calling tensor to contain only a single element, as is the case when computing a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b176076",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95eaea",
   "metadata": {},
   "source": [
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its next_functions property. We can see below that drilling down on this property on `d` shows us the gradient functions for all the prior tensors. Note that `a.grad_fn` is reported as None, indicating that this was an input to the function with no history of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07523cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('d:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fe2ad",
   "metadata": {},
   "source": [
    "Call the `backward()` method on the output, and check the input’s `grad` property to inspect the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6b8ad",
   "metadata": {},
   "source": [
    "Let’s define a small model and examine how it changes after a single training batch. First, define a few constants, our model, and some stand-ins for inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23829989",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(1000, 100)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa025c93",
   "metadata": {},
   "source": [
    "One thing you might notice is that we never specify `requires_grad=True` for the model’s layers. Within a subclass of `torch.nn.Module`, it’s assumed that we want to track gradients on the layers’ weights for learning.\n",
    "\n",
    "If we look at the layers of the model, we can examine the values of the weights, and verify that no gradients have been computed yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a43baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layer2.weight[0,:10])\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07929116",
   "metadata": {},
   "source": [
    "Compute the prediction and $L^2$-loss on `some_input`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(some_input)\n",
    "loss = (ideal_output - pred).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f716d75",
   "metadata": {},
   "source": [
    "Compute the gradients via backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8d7cf",
   "metadata": {},
   "source": [
    "We can see that the gradients have been computed for each learning weight, but the weights remain unchanged, because we haven’t run the optimizer yet.\n",
    "\n",
    "Initialize and run the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f919b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beabcd4",
   "metadata": {},
   "source": [
    "Now the weights in `layer2` are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1347c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5ecf2",
   "metadata": {},
   "source": [
    "One important thing about the process: After calling optimizer.step(), you need to call optimizer.zero_grad(), or else every time you run loss.backward(), the gradients on the learning weights will accumulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "for i in range(0, 5):\n",
    "    prediction = model(some_input)\n",
    "    loss = (ideal_output - prediction).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "optim.zero_grad()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f227eb",
   "metadata": {},
   "source": [
    "After running the cell above, you should see that after running loss.backward() multiple times, the magnitudes of most of the gradients will be much larger. Failing to zero the gradients before running your next training batch will cause the gradients to blow up in this manner, causing incorrect and unpredictable learning results.\n",
    "\n",
    "The simplest way to enable / disable gradient tracking is to change the `required_grad` flag on a tensor directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0479b",
   "metadata": {},
   "source": [
    "If you only need autograd turned off temporarily, a better way is to use the torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5474500",
   "metadata": {},
   "source": [
    "`torch.no_grad()` can also be used as a function or method dectorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726eecb0",
   "metadata": {},
   "source": [
    "Autograd tracks every step of your computation in detail. Such a computation history, combined with timing information, would make a handy profiler - and autograd has that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7265fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "\n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c1df3",
   "metadata": {},
   "source": [
    "There is an API on autograd that allows you to calculate the Jacobian and the Hessian matrices of a particular function for particular inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5a6fc",
   "metadata": {},
   "source": [
    "The `torch.autograd.functional.hessian()` method works identically (assuming your function is twice differentiable), but returns a matrix of all second derivatives.\n",
    "\n",
    "## 5. Data manipulation layers\n",
    "Max pooling reduces a tensor by combining cells, and assigning the maximum value of the input cells to the output cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204abd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f07dde",
   "metadata": {},
   "source": [
    "Normalization layers re-center and normalize the output of one layer before feeding it to another. Centering the and scaling the intermediate tensors has a number of beneficial effects, such as letting you use higher learning rates without exploding/vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b35ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a4c9f",
   "metadata": {},
   "source": [
    "Dropout layers work by randomly setting parts of the input tensor to zero during training - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ed6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
