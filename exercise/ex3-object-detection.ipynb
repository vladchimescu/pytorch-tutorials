{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0301308",
   "metadata": {},
   "source": [
    "# Object detection with Mask R-CNN in PyTorch\n",
    "\n",
    "We will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab12bc",
   "metadata": {},
   "source": [
    "## Visualization utilities in PyTorch\n",
    "First, let's explore how we can visualize bounding boxes and segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "\n",
    "dog1_int = read_image(str(Path('data/assets') / 'dog1.jpg'))\n",
    "dog2_int = read_image(str(Path('data/assets') / 'dog2.jpg'))\n",
    "\n",
    "grid = make_grid([dog1_int, dog2_int, dog1_int, dog2_int])\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b95b0",
   "metadata": {},
   "source": [
    "We can plot bounding boxes produced by torchvision detection models. Here is demo with a Faster R-CNN model loaded from `fasterrcnn_resnet50_fpn()` model. We could also try using a RetinaNet with `retinanet_resnet50_fpn()`, an SSDlite with `ssdlite320_mobilenet_v3_large()` or an SSD with `ssd300_vgg16()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b35d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "\n",
    "batch_int = torch.stack([dog1_int, dog2_int])\n",
    "batch = convert_image_dtype(batch_int, dtype=torch.float)\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "outputs = model(batch)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a31c3",
   "metadata": {},
   "source": [
    "Let’s plot the boxes detected by our model. We will only plot the boxes with a score greater than a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = .8\n",
    "dogs_with_boxes = [\n",
    "    draw_bounding_boxes(dog_int, boxes=output['boxes'][output['scores'] > score_threshold], width=4)\n",
    "    for dog_int, output in zip(batch_int, outputs)\n",
    "]\n",
    "show(dogs_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd97f5",
   "metadata": {},
   "source": [
    "The `draw_segmentation_masks()` function can be used to draw segmentation masks on images. Semantic segmentation and instance segmentation models have different outputs, so we will treat each independently.\n",
    "\n",
    "We will see how to use it with torchvision’s FCN Resnet-50, loaded with `fcn_resnet50()`. You can also try using DeepLabv3 (`deeplabv3_resnet50()`) or lraspp mobilenet models (`lraspp_mobilenet_v3_large()`).\n",
    "\n",
    "Let’s start by looking at the output of the model. Remember that in general, images must be normalized before they’re passed to a semantic segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "model = fcn_resnet50(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "normalized_batch = F.normalize(batch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "output = model(normalized_batch)['out']\n",
    "print(output.shape, output.min().item(), output.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757ab01",
   "metadata": {},
   "source": [
    "As we can see above, the output of the segmentation model is a tensor of shape `(batch_size, num_classes, H, W)`. Each value is a non-normalized score, and we can normalize them into `[0, 1]` by using a softmax. After the softmax, we can interpret each value as a probability indicating how likely a given pixel is to belong to a given class.\n",
    "\n",
    "Let’s plot the masks that have been detected for the dog class and for the boat class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a761e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_classes = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
    "\n",
    "normalized_masks = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "dog_and_boat_masks = [\n",
    "    normalized_masks[img_idx, sem_class_to_idx[cls]]\n",
    "    for img_idx in range(batch.shape[0])\n",
    "    for cls in ('dog', 'boat')\n",
    "]\n",
    "\n",
    "show(dog_and_boat_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc66464",
   "metadata": {},
   "source": [
    "The `draw_segmentation_masks()` function can be used to plots those masks on top of the original image. This function expects the masks to be boolean masks, but our masks above contain probabilities in `[0, 1]`. To get boolean masks, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dim = 1\n",
    "boolean_dog_masks = (normalized_masks.argmax(class_dim) == sem_class_to_idx['dog'])\n",
    "print(f\"shape = {boolean_dog_masks.shape}, dtype = {boolean_dog_masks.dtype}\")\n",
    "show([m.float() for m in boolean_dog_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1e07",
   "metadata": {},
   "source": [
    "Now that we have boolean masks, we can use them with `draw_segmentation_masks()` to plot them on top of the original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "dogs_with_masks = [\n",
    "    draw_segmentation_masks(img, masks=mask, alpha=0.7)\n",
    "    for img, mask in zip(batch_int, boolean_dog_masks)\n",
    "]\n",
    "show(dogs_with_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b4a8e",
   "metadata": {},
   "source": [
    "We can plot more than one mask per image. Remember that the model returned as many masks as there are classes. Let’s ask the same query as above, but this time for all classes, not just the dog class: “For each pixel and each class C, is class C the most most likely class?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ab315",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = normalized_masks.shape[1]\n",
    "dog1_masks = normalized_masks[0]\n",
    "class_dim = 0\n",
    "dog1_all_classes_masks = dog1_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None]\n",
    "\n",
    "print(f\"dog1_masks shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}\")\n",
    "print(f\"dog1_all_classes_masks = {dog1_all_classes_masks.shape}, dtype = {dog1_all_classes_masks.dtype}\")\n",
    "\n",
    "dog_with_all_masks = draw_segmentation_masks(dog1_int, masks=dog1_all_classes_masks, alpha=.6)\n",
    "show(dog_with_all_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbc986",
   "metadata": {},
   "source": [
    "We can see in the image above that only 2 masks were drawn: the mask for the background and the mask for the dog. This is because the model thinks that only these 2 classes are the most likely ones across all the pixels. If the model had detected another class as the most likely among other pixels, we would have seen its mask above.\n",
    "\n",
    "Removing the background mask is as simple as passing `masks=dog1_all_classes_masks[1:]`, because the background class is the class with index 0.\n",
    "\n",
    "Let’s now do the same but for an entire batch of images. The code is similar but involves a bit more juggling with the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dim = 1\n",
    "all_classes_masks = normalized_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None, None]\n",
    "print(f\"shape = {all_classes_masks.shape}, dtype = {all_classes_masks.dtype}\")\n",
    "# The first dimension is the classes now, so we need to swap it\n",
    "all_classes_masks = all_classes_masks.swapaxes(0, 1)\n",
    "\n",
    "dogs_with_masks = [\n",
    "    draw_segmentation_masks(img, masks=mask, alpha=.6)\n",
    "    for img, mask in zip(batch_int, all_classes_masks)\n",
    "]\n",
    "show(dogs_with_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab97dda",
   "metadata": {},
   "source": [
    "Instance segmentation models have a significantly different output from the semantic segmentation models. We will see here how to plot the masks for such models. Let’s start by analyzing the output of a Mask-RCNN model. **Note that these models don’t require the images to be normalized**, so we don’t need to use the normalized batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "output = model(batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692acf9e",
   "metadata": {},
   "source": [
    "Let’s break this down. For each image in the batch, the model outputs some detections (or instances). The number of detections varies for each input image. Each instance is described by its bounding box, its label, its score and its mask.\n",
    "\n",
    "The way the output is organized is as follows: the output is a list of length `batch_size`. Each entry in the list corresponds to an input image, and it is a `dict` with keys ‘boxes’, ‘labels’, ‘scores’, and ‘masks’. Each value associated to those keys has `num_instances` elements in it. In our case above there are 4 instances detected in the first image, and 3 instances in the second one.\n",
    "\n",
    "The boxes can be plotted with `draw_bounding_boxes()` as above, but here we’re more interested in the masks. These masks are quite different from the masks that we saw above for the semantic segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2743a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog1_output = output[0]\n",
    "dog1_masks = dog1_output['masks']\n",
    "print(f\"shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}, \"\n",
    "      f\"min = {dog1_masks.min()}, max = {dog1_masks.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849b625",
   "metadata": {},
   "source": [
    "Here the masks corresponds to probabilities indicating, for each pixel, how likely it is to belong to the predicted label of that instance. Those predicted labels correspond to the ‘labels’ element in the same output dict. Let’s see which labels were predicted for the instances of the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42294611",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "inst_class_to_idx = {cls: idx for (idx, cls) in enumerate(inst_classes)}\n",
    "\n",
    "print(\"For the first dog, the following instances were detected:\")\n",
    "print([inst_classes[label] for label in dog1_output['labels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923ecb9",
   "metadata": {},
   "source": [
    "Interestingly, the model detects 3 dogs and a donut in the image. Let’s go ahead and plot those masks. Since `draw_segmentation_masks()` expects boolean masks, we need to convert those probabilities into boolean values. Remember that the semantic of those masks is “How likely is this pixel to belong to the predicted class?”. As a result, a natural way of converting those masks into boolean values is to threshold them with the 0.5 probability (one could also choose a different threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_threshold = 0.5\n",
    "dog1_bool_masks = dog1_output['masks'] > proba_threshold\n",
    "print(f\"shape = {dog1_bool_masks.shape}, dtype = {dog1_bool_masks.dtype}\")\n",
    "\n",
    "# There's an extra dimension (1) to the masks. We need to remove it\n",
    "dog1_bool_masks = dog1_bool_masks.squeeze(1)\n",
    "\n",
    "show(draw_segmentation_masks(dog1_int, dog1_bool_masks, alpha=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def42a83",
   "metadata": {},
   "source": [
    "Let's check the scores for class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dog1_output['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c174",
   "metadata": {},
   "source": [
    "The scores for 'donut' and the third dog are low and we can remove these instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = .75\n",
    "\n",
    "boolean_masks = [\n",
    "    out['masks'][out['scores'] > score_threshold] > proba_threshold\n",
    "    for out in output\n",
    "]\n",
    "\n",
    "dogs_with_masks = [\n",
    "    draw_segmentation_masks(img, mask.squeeze(1))\n",
    "    for img, mask in zip(batch_int, boolean_masks)\n",
    "]\n",
    "show(dogs_with_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ce025",
   "metadata": {},
   "source": [
    "## Penn-Fudan Database for Pedestrian Detection and Segmentation\n",
    "\n",
    "First, download and extract the zip [file](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip). Let's explore the structure of the dataset directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccecfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls PennFudanPed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25788276",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls PennFudanPed/Annotation/ | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls PennFudanPed/PNGImages/ | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls PennFudanPed/PedMasks/ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6e478",
   "metadata": {},
   "source": [
    "Load an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf53f0",
   "metadata": {},
   "source": [
    "Load a sample segmentation mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ecc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')\n",
    "# each mask instance has a different color, from zero to N, where\n",
    "# N is the number of instances. In order to make visualization easier,\n",
    "# let's adda color palette to the mask.\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ac916",
   "metadata": {},
   "source": [
    "Write a custom `Dataset` class for the Penn-Fudan pedestrian data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e77704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        \n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f7585",
   "metadata": {},
   "source": [
    "## Mask R-CNN\n",
    "We will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image. Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n",
    "\n",
    "There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
    "\n",
    "## Finetuning from a pretrained model\n",
    "\n",
    "Suppose we want to start from a model pre-trained on COCO and want to finetune it for our particular classes. Here is a possible way of doing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1978548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bfe416",
   "metadata": {},
   "source": [
    "We can also modify the model to add a different backbone. For example, we can replace the ResNet-50 with MobileNet v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features (only convolutional layer activations)\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb623f2",
   "metadata": {},
   "source": [
    "In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small, so we will be following approach number 1.\n",
    "\n",
    "Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852eab4",
   "metadata": {},
   "source": [
    "Let’s write some helper functions for data augmentation / transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ffd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76abfff",
   "metadata": {},
   "source": [
    "Before iterating over the dataset, it’s good to see what the model expects during training and inference time on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('detection/')\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    " collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74360b6a",
   "metadata": {},
   "source": [
    "We now have the dataset class, the models and the data transforms. Let's instantiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521335f",
   "metadata": {},
   "source": [
    "Now let's instantiate the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054929b",
   "metadata": {},
   "source": [
    "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38301f3",
   "metadata": {},
   "source": [
    "Now that training has finished, let's have a look at what it actually predicts in a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ff150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e46fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d81ec",
   "metadata": {},
   "source": [
    "Let's inspect the image and the predicted segmentation masks. For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc81451",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1645383",
   "metadata": {},
   "source": [
    "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where N is the number of predictions, and are probability maps between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
