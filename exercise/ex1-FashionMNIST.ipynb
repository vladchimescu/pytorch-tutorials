{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ed798f",
   "metadata": {},
   "source": [
    "# Multiclass Classification on Fashion MNIST\n",
    "February 2, 2022\n",
    "\n",
    "*Vladislav Kim*\n",
    "\n",
    "In this notebook we will explore multiclass classification on Fashion MNIST dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc011deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c345129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0dd95",
   "metadata": {},
   "source": [
    "First, download the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba80f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # converts a PIL image or np.array into a FloatTensor\n",
    "                          # and scales the image's pixel intensity values in the range (0,1)\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854b9d1",
   "metadata": {},
   "source": [
    "Specify batch size and pass the `Dataset` object to `DataLoader` to create an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036050c",
   "metadata": {},
   "source": [
    "Check the dimensions of $X$ and $y$ `(num_instances, n_channels, height, width)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fc4aa",
   "metadata": {},
   "source": [
    "We can create a lambda transform for target variables that converts an integer vector into one-hot representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb478bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5108a83",
   "metadata": {},
   "source": [
    "Iterate and visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a871c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "torch.manual_seed(32)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622caf27",
   "metadata": {},
   "source": [
    "## 1. Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ef806",
   "metadata": {},
   "source": [
    "Generate toy data with 3 instances (the first dimension is the batch axis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c063430",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(32)\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390f9bf",
   "metadata": {},
   "source": [
    "We will use a simple fully connected network. Let's explore individual layers of this simple network. The input layer flattens an image. `nn.Flatten` unwraps the 2D array into a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d79ffc",
   "metadata": {},
   "source": [
    "The first hidden layer (`nn.Linear`) applies a linear transformation on the input using its stored weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879db27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91fed3",
   "metadata": {},
   "source": [
    "A non-linear activation function is applied before passing to the next layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6ba5f",
   "metadata": {},
   "source": [
    "`nn.Sequential` can be used to define a sequence of layers. The data is passed through all the modules in the same order as defined. It can be used to put together a quick network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c76560",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=28*28, out_features=20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=20, out_features=10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67d8a3",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network returns `logits` - raw values in $[-\\infty, \\infty]$ - which are passed to the `nn.Softmax` module. The logits are scaled to values\n",
    "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
    "which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted probabilities: \", pred_probab[0])\n",
    "print(\"Which sum to %.1f\" % pred_probab[0].sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e9cc3",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from `nn.Module`. We define the layers of the network\n",
    "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
    "operations in the neural network, we move it to the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27fc80",
   "metadata": {},
   "source": [
    "## 2. Train and assess the model\n",
    "Specify a loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfde57",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
    "backpropagates the prediction error to adjust the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        # reset gradients (otherwie they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # run backpropagation\n",
    "        loss.backward()\n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb0e4e",
   "metadata": {},
   "source": [
    "We also check the model's performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    # don't track the gradients\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # forward pass\n",
    "            pred = model(X)\n",
    "            # aggregate loss\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # average loss\n",
    "    test_loss /= num_batches\n",
    "    # accuracy\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cab49d",
   "metadata": {},
   "source": [
    "The training process is conducted over several epochs. During each epoch, the model learns\n",
    "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
    "accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb272bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
